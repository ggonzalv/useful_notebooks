{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizaje por Transferencia\n",
    "\n",
    "Las redes neuronales tienen complicaciones para ser entrenadas de cero, pues requieren muchos datos y muchas GPUs. Esto es así en tanto que el lenguaje es muy complejo, y se requiere de una profunda base de datos para coger los matices. Entrenar un modelo de cero en NLP es muy costoso.\n",
    "\n",
    "El **aprendizaje por transferencia** nos permite ahorrarnos parcialmente el coste de entrenamiento. La red neuronal se parte en dos objetos: el **cuerpo** y la **cabeza**. Las capas del cuerpo tienen unos pesos que entienden cómo funciona el lenguaje. Estos pesos pueden ser reutilizados para inicializar un nuevo modelo, con lo cual no hace falta que el nuevo modelo aprenda desde cero las características más básicas del lenguaje. La cabeza, por otro lado, contiene las capas que se focalizan en un problema en particular.\n",
    "\n",
    "**Modelado del lenguaje**:\n",
    "- Autorregresivo: Aprender cuál es la siguiente palabra dada una secuencia. My name --> is\n",
    "- Enmascarado: Aprender una palabra \"máscara\" dada un contesto. My MASK name is ...\n",
    "\n",
    "Con este concepto del modelado del lenguaje ya podemos entrenar un modelo: **pre-entrenamiento**. Entrenamos el modelo desde cero (muchos datos e.g. wikipedia, mucha computación). Después, con este *pretrained language model* (e.g. BERT) ya podemos hacer muchas tareas. \n",
    "\n",
    "**Fine-tuning**: Utilizamos los pesos del modelo pre-entrenado (cuerpo), y añadimos una cabeza específica para la tarea. Los pesos del cuerpo y sobre todo los de la cabeza (que son random en origen) son tuneados después usando *gradient descent*, dando pie a un *fine-tuned language model*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"aprendizaje_transferencia.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (4.16.2)\n",
      "Requirement already satisfied: datasets in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (2.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: sacremoses in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: requests in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: filelock in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: importlib-metadata in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from transformers) (4.11.3)\n",
      "Requirement already satisfied: multiprocess in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from datasets) (7.0.0)\n",
      "Requirement already satisfied: aiohttp in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from datasets) (2022.3.0)\n",
      "Requirement already satisfied: pandas in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from datasets) (1.1.3)\n",
      "Requirement already satisfied: xxhash in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: responses<0.19 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: dill in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from aiohttp->datasets) (20.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from pandas->datasets) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from sacremoses->transformers) (0.17.0)\n"
     ]
    }
   ],
   "source": [
    "#Install hugging face libraries\n",
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to /Users/galogonzalvo/.huggingface/token\n"
     ]
    }
   ],
   "source": [
    "#Login to huggingface. You will need to sign up and create a token. Follow https://huggingface.co/\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "        To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/token.\n",
      "        (Deprecated, will be removed in v0.3.0) To login with username and password instead, interrupt with Ctrl+C.\n",
      "        \n",
      "Username: "
     ]
    }
   ],
   "source": [
    "#Nos logamos a huggingface AI donde hay muchos datasets y modelos pre-entrenados\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The operation couldn’t be completed. Unable to locate a Java Runtime.\r\n",
      "Please visit http://www.java.com for information on installing Java.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!apt install git-lfs\n",
    "!git config --global user.email \"galo.gonzalvo@gmail.com\"\n",
    "!git config --global user.name \"Galo Gonzalvo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9718f700d6bd450d92a3d046055b4315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce33c7ad3a8443478576cea3d5bf8807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/3.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset amazon_reviews_multi/es (download: 77.58 MiB, generated: 52.44 MiB, post-processed: Unknown size, total: 130.02 MiB) to /Users/galogonzalvo/.cache/huggingface/datasets/amazon_reviews_multi/es/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789ef36dc6f24b6a8481da1ab116b17b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae27ed58f9ed4342abf66674fcbdd873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/77.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982b3f416ea74027b66e40f791a61ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0f09ac4a8a46bcb5cfd7e3c62cf352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7596a889d14300934e734e86725015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.93M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fed2fe07a3e469bbcfea540c01ff270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e618dda5fd4f41a1aeb8723d412f4099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b844b4ace1eb44378c53e0e5aae2ebbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0986047dd860499282e3f15608146181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/200000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset amazon_reviews_multi downloaded and prepared to /Users/galogonzalvo/.cache/huggingface/datasets/amazon_reviews_multi/es/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b559af96e441ffa46a3b2fb6b78989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 200000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"amazon_reviews_multi\",\"es\") #download amazon review data in spanish\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos un train, validation y test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_title</th>\n",
       "      <th>language</th>\n",
       "      <th>product_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>es_0655494</td>\n",
       "      <td>product_es_0746693</td>\n",
       "      <td>reviewer_es_0751863</td>\n",
       "      <td>3</td>\n",
       "      <td>Bueno aceptable lo compre para hacer un champu natural y me va bien aunque no hace mucha espuma</td>\n",
       "      <td>BIEN</td>\n",
       "      <td>es</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>es_0746591</td>\n",
       "      <td>product_es_0377694</td>\n",
       "      <td>reviewer_es_0971770</td>\n",
       "      <td>4</td>\n",
       "      <td>Todo muy bien, lo único que aunque actives las ventosas tienes que sujetarlo porque pierde presión y acaba cayendo, aún así cumple su función.</td>\n",
       "      <td>Funciona correctamente</td>\n",
       "      <td>es</td>\n",
       "      <td>home_improvement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>es_0551590</td>\n",
       "      <td>product_es_0881435</td>\n",
       "      <td>reviewer_es_0251012</td>\n",
       "      <td>1</td>\n",
       "      <td>Si lo quieres utilizar sin conectarlo al cable de antena podras ver pocos canales ó ninguno.</td>\n",
       "      <td>Inhalambrico va regular</td>\n",
       "      <td>es</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>es_0907056</td>\n",
       "      <td>product_es_0010697</td>\n",
       "      <td>reviewer_es_0819996</td>\n",
       "      <td>1</td>\n",
       "      <td>Leí en las preguntas que se realizan que valía para una bateria olympus omd em10 mark II, y no vale.</td>\n",
       "      <td>No vale o para olympus omd em10 II Mark</td>\n",
       "      <td>es</td>\n",
       "      <td>electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>es_0192218</td>\n",
       "      <td>product_es_0742022</td>\n",
       "      <td>reviewer_es_0107865</td>\n",
       "      <td>4</td>\n",
       "      <td>He tenido que comprar el producto dos veces. La primera me llegaron únicamente 4 piezas de las 7 anunciadas, además, dos de ellas rotas. Era un reacondicionado. Devolución y compro de nuevo. Esta vez han llegado todas, compré directamente SIN reacondicionado. Han llegado las 7 bien, aunque una de ellas tiene una ralla bastante grande, pero se puede utilizar.</td>\n",
       "      <td>Segunda compra, la primera mal estado</td>\n",
       "      <td>es</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>es_0206869</td>\n",
       "      <td>product_es_0867953</td>\n",
       "      <td>reviewer_es_0536938</td>\n",
       "      <td>5</td>\n",
       "      <td>Esta bien, es lo que me esperaba, lo he usado para poner encima de una tarta.</td>\n",
       "      <td>Bien</td>\n",
       "      <td>es</td>\n",
       "      <td>toy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>es_0535879</td>\n",
       "      <td>product_es_0527386</td>\n",
       "      <td>reviewer_es_0803740</td>\n",
       "      <td>1</td>\n",
       "      <td>No aguantan mucho en zonas humedas. Se me desprenden de la ducha en muy poco tiempo y he probado a ponerlos de varias formas. :(</td>\n",
       "      <td>Se desprenden con facilidad</td>\n",
       "      <td>es</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>es_0350497</td>\n",
       "      <td>product_es_0430493</td>\n",
       "      <td>reviewer_es_0941148</td>\n",
       "      <td>1</td>\n",
       "      <td>el producto bien, pero me lo han cobrado dos veces</td>\n",
       "      <td>cobro duplicado</td>\n",
       "      <td>es</td>\n",
       "      <td>pet_products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>es_0579291</td>\n",
       "      <td>product_es_0200458</td>\n",
       "      <td>reviewer_es_0295203</td>\n",
       "      <td>5</td>\n",
       "      <td>YO NO TENGO PERROS SINO GATOS PERO EL DE GATOS COSTABA BASTANTE MAS EN AQUEL MOMENTO DECORATIVO SIEMPRE Y CUANDO TENGAS OTRO MAS GRANDE PAA ALMACENAJE</td>\n",
       "      <td>MUY BONITO</td>\n",
       "      <td>es</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>es_0698172</td>\n",
       "      <td>product_es_0142466</td>\n",
       "      <td>reviewer_es_0054780</td>\n",
       "      <td>1</td>\n",
       "      <td>No puedo opinar porque no he recibido el artículo ni lo voy a recibir, cuando compras a vendedores externos si hay algún problema pasan de todo ya es la segunda vez que me timan en Amazon.</td>\n",
       "      <td>Timo</td>\n",
       "      <td>es</td>\n",
       "      <td>pc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Import packages\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "from datasets import ClassLabel\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "#Función para mostrar elementos aleatorios del dataset\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    \"Taken from https://github.com/huggingface/notebooks/blob/master/examples/text_classification.ipynb\"\n",
    "    \n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a construir un modelo de tal forma que si le pasamos un review, sea capaz de decirnos si es positivo o negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_title</th>\n",
       "      <th>language</th>\n",
       "      <th>product_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>es_0491108</td>\n",
       "      <td>product_es_0296024</td>\n",
       "      <td>reviewer_es_0999081</td>\n",
       "      <td>1</td>\n",
       "      <td>Nada bueno se me fue ka pantalla en menos de 8...</td>\n",
       "      <td>television Nevir</td>\n",
       "      <td>es</td>\n",
       "      <td>electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>es_0869872</td>\n",
       "      <td>product_es_0922286</td>\n",
       "      <td>reviewer_es_0216771</td>\n",
       "      <td>1</td>\n",
       "      <td>Horrible, nos tuvimos que comprar otro porque ...</td>\n",
       "      <td>Dinero tirado a la basura con esta compra</td>\n",
       "      <td>es</td>\n",
       "      <td>electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>es_0811721</td>\n",
       "      <td>product_es_0474543</td>\n",
       "      <td>reviewer_es_0929213</td>\n",
       "      <td>1</td>\n",
       "      <td>Te obligan a comprar dos unidades y te llega s...</td>\n",
       "      <td>solo llega una unidad cuando te obligan a comp...</td>\n",
       "      <td>es</td>\n",
       "      <td>drugstore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>es_0359921</td>\n",
       "      <td>product_es_0656090</td>\n",
       "      <td>reviewer_es_0224702</td>\n",
       "      <td>1</td>\n",
       "      <td>No entro en descalificar al vendedor, solo pue...</td>\n",
       "      <td>PRODUCTO NO RECIBIDO.</td>\n",
       "      <td>es</td>\n",
       "      <td>wireless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>es_0068940</td>\n",
       "      <td>product_es_0662544</td>\n",
       "      <td>reviewer_es_0224827</td>\n",
       "      <td>1</td>\n",
       "      <td>Llega tarde y co la talla equivocada</td>\n",
       "      <td>Devuelto</td>\n",
       "      <td>es</td>\n",
       "      <td>shoes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    review_id          product_id          reviewer_id  stars  \\\n",
       "0  es_0491108  product_es_0296024  reviewer_es_0999081      1   \n",
       "1  es_0869872  product_es_0922286  reviewer_es_0216771      1   \n",
       "2  es_0811721  product_es_0474543  reviewer_es_0929213      1   \n",
       "3  es_0359921  product_es_0656090  reviewer_es_0224702      1   \n",
       "4  es_0068940  product_es_0662544  reviewer_es_0224827      1   \n",
       "\n",
       "                                         review_body  \\\n",
       "0  Nada bueno se me fue ka pantalla en menos de 8...   \n",
       "1  Horrible, nos tuvimos que comprar otro porque ...   \n",
       "2  Te obligan a comprar dos unidades y te llega s...   \n",
       "3  No entro en descalificar al vendedor, solo pue...   \n",
       "4               Llega tarde y co la talla equivocada   \n",
       "\n",
       "                                        review_title language product_category  \n",
       "0                                   television Nevir       es      electronics  \n",
       "1          Dinero tirado a la basura con esta compra       es      electronics  \n",
       "2  solo llega una unidad cuando te obligan a comp...       es        drugstore  \n",
       "3                              PRODUCTO NO RECIBIDO.       es         wireless  \n",
       "4                                           Devuelto       es            shoes  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.set_format(\"pandas\") # es muy fácil convertir un dataset a pandas\n",
    "df = dataset[\"train\"][:] # seleccionas todos los rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "home                        26962\n",
       "wireless                    25886\n",
       "toy                         13647\n",
       "sports                      13189\n",
       "pc                          11191\n",
       "home_improvement            10879\n",
       "electronics                 10385\n",
       "beauty                       7337\n",
       "automotive                   7143\n",
       "kitchen                      6695\n",
       "apparel                      5737\n",
       "drugstore                    5513\n",
       "book                         5264\n",
       "furniture                    5229\n",
       "baby_product                 4881\n",
       "office_product               4771\n",
       "lawn_and_garden              4237\n",
       "other                        3937\n",
       "pet_products                 3713\n",
       "personal_care_appliances     3573\n",
       "luggage                      3328\n",
       "camera                       3029\n",
       "shoes                        2754\n",
       "digital_ebook_purchase       1843\n",
       "video_games                  1733\n",
       "jewelry                      1598\n",
       "musical_instruments          1530\n",
       "watch                        1490\n",
       "industrial_supplies          1482\n",
       "grocery                      1044\n",
       "Name: product_category, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['product_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    40000\n",
       "4    40000\n",
       "3    40000\n",
       "2    40000\n",
       "1    40000\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['stars'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.reset_format() #volver al dataset object original si lo necesitas. muy fácil cambiar de formato con esta librería"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_neutral_stars(examples):\n",
    "    return examples[\"stars\"] != 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b2eefd8d674170b0132748683a359a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6aa679dae6841ba81d1317343c559cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91e4e43fcaa4c36826c621e11ea01a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"stars\"] != 3) # no nos interesan los neutrales, queremos positivo o negativo. esta es la manera más eficiente de hacerlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a juntar las valoraciones en positiva o negativa\n",
    "def merge_star_ratings(examples):\n",
    "    if examples[\"stars\"] <= 2:\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    return {\"labels\": label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b1df939623475991f67cf0bacbc054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13bb9f5f52844c1bae8d86745ec2b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659f06494cd146e1a3cf4b4929473152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(merge_star_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_title</th>\n",
       "      <th>language</th>\n",
       "      <th>product_category</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>es_0712424</td>\n",
       "      <td>product_es_0694461</td>\n",
       "      <td>reviewer_es_0045033</td>\n",
       "      <td>2</td>\n",
       "      <td>Para ser tan Caro no esperéis grandísima calidad, una cosa normal, estéticamente tampoco convence no se si sería por el color elegido, pero no queda como esperas es demasiada fina la correa. El pedido tardo más de lo esperado.</td>\n",
       "      <td>No lo volvería a comprar</td>\n",
       "      <td>es</td>\n",
       "      <td>wireless</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>es_0140956</td>\n",
       "      <td>product_es_0780994</td>\n",
       "      <td>reviewer_es_0609507</td>\n",
       "      <td>1</td>\n",
       "      <td>Este tamaño se me rompió dos veces. Sin embargo la de 5cms sigue viva después de utilizarla bastante.</td>\n",
       "      <td>Va mejor la perforadora de 5cm de diámetro</td>\n",
       "      <td>es</td>\n",
       "      <td>toy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>es_0367959</td>\n",
       "      <td>product_es_0568071</td>\n",
       "      <td>reviewer_es_0806838</td>\n",
       "      <td>1</td>\n",
       "      <td>Poca Calidad. Una semana de uso y ya se están abriendo las costuras.</td>\n",
       "      <td>Poco recomendable si no pesa 50 kg</td>\n",
       "      <td>es</td>\n",
       "      <td>lawn_and_garden</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"], num_examples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primero vamos a tokenizar las reseñas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ccc6553c6464d909557c8149f7f51e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29137a09ce848b4aab76a2565686f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb83d84428a47819d0cc648824e9165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97509fb40884ba2bcd206832df8ba57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/497k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceef48433734456698f5213fc1aeeb75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e829e577f9d49489a1d14f653e51f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"BSC-TeMU/roberta-base-bne\" #Cargamos un modelo pre-entrenado de hugging face. Puedes mirar este modelo en https://huggingface.co/models\n",
    "#Este se basa en un modelo fill-mask (modelado de lenguaje enmascarado) de \"roberta\", que es un modelo entrenado algo más poderoso que BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50262"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size # tamaño del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <s>\n",
      "1465 ¡\n",
      "12616 hola\n",
      "66 ,\n",
      "503  me\n",
      "17111  llamo\n",
      "532  le\n",
      "19514 wis\n",
      "55 !\n",
      "2 </s>\n"
     ]
    }
   ],
   "source": [
    "#tokenizamos y convertimos a número\n",
    "\n",
    "text = \"¡hola, me llamo lewis!\"\n",
    "tokenized_text = tokenizer.encode(text) \n",
    "\n",
    "for token in tokenized_text:\n",
    "    print(token, tokenizer.decode([token])) #(id de palabra, palabra). \n",
    "    \n",
    "#Lewis lo ha roto en 2 porque lewis no es común en español. Los tokens <s> </s> indican principio y final de frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  1465, 12616,    66,   503, 17111,   532, 19514,    55,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = tokenizer(text,return_tensors=\"pt\") #el output es un tensor de pytorch\n",
    "encoded_text \n",
    "#ids de cada token, attention mask es una manera de indicar al modelo qué partes de la frase debe aplicar atención o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplica el tokenizador a cada row en nuestro dataset\n",
    "def tokenize_reviews(examples):\n",
    "    return tokenizer(examples[\"review_body\"], truncation=True) \n",
    "#Truncation sirve para truncar un texto exageradamente grande que exceda las dimensiones pre-establecidas de los vectores de nuestro modelo. En BERT, el vector tiene unas dimensiones máximas de 758"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122e827b858f435c90fd1b254582b786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e2478a47dd46b28ba14acec5c44b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2b5c72e6424425b088011ae4237cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 160000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = dataset[\"train\"].column_names\n",
    "columns.remove(\"labels\")\n",
    "encoded_dataset = dataset.map(tokenize_reviews, batched=True, remove_columns=columns) #remove columns quita las columnas que no interesan para simplificar\n",
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': 0,\n",
       " 'input_ids': [0,\n",
       "  10626,\n",
       "  3383,\n",
       "  361,\n",
       "  503,\n",
       "  847,\n",
       "  36181,\n",
       "  4747,\n",
       "  334,\n",
       "  1111,\n",
       "  313,\n",
       "  1369,\n",
       "  1635,\n",
       "  342,\n",
       "  403,\n",
       "  1594,\n",
       "  4162,\n",
       "  2957,\n",
       "  369,\n",
       "  10925,\n",
       "  2],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ya tenemos los textos preparados. Cargamos el modelo pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2627fedcae094b579291e29a446d4529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/476M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at BSC-TeMU/roberta-base-bne were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at BSC-TeMU/roberta-base-bne and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Text classification es un caso particular de sequence classification.\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.0917, -0.1284]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**encoded_text) #** stands for kwargs. keywoard arguments passed as a dictionary of arbitrary length\n",
    "outputs\n",
    "#el output son dos logits, uno para sentimiento positivo y otro para negativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definimos las métricas para evaluar el rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-macosx_10_13_x86_64.whl (7.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.8 MB 5.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from scikit-learn->sklearn) (0.17.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=a4241dbc58916f533d4ff57ab6c639fc6fd4100e3658681607069d389b8fe77a\n",
      "  Stored in directory: /Users/galogonzalvo/Library/Caches/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, scikit-learn, sklearn\n",
      "Successfully installed scikit-learn-1.0.2 sklearn-0.0 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"accuracy\", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: \"\"\"\n",
       "Args:\n",
       "    predictions: Predicted labels, as returned by a model.\n",
       "    references: Ground truth labels.\n",
       "    normalize: If False, return the number of correctly classified samples.\n",
       "        Otherwise, return the fraction of correctly classified samples.\n",
       "    sample_weight: Sample weights.\n",
       "Returns:\n",
       "    accuracy: Accuracy score.\n",
       "Examples:\n",
       "\n",
       "    >>> accuracy_metric = datasets.load_metric(\"accuracy\")\n",
       "    >>> results = accuracy_metric.compute(references=[0, 1], predictions=[0, 1])\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Construimos una función que nos devuelve las métricas a mitad de entrenamiento\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions,axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pasamos al entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/galogonzalvo/opt/anaconda3/envs/tf/lib/python3.7/site-packages/transformers/training_args.py:951: FutureWarning: `--push_to_hub_model_id` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_model_id` instead and pass the full repo name to this argument (in this case ggonzalv/roberta-base-bne-finetuned-amazon_reviews_multi).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "#En transformers tenemos toda la lógica de entrenamiento en un lugar (paso a paso, cambiar los pesos, actualizar la logloss usando gradient descent, etc.)\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "batch_size = 16 #cuántos ejemplos vamos a pasar al modelo en cada iteración\n",
    "num_train_epochs=2\n",
    "num_train_samples = 20_000 #subset de los datos para que se entrene en 10min\n",
    "train_dataset = encoded_dataset[\"train\"].shuffle(seed=42).select(range(num_train_samples))\n",
    "logging_steps = len(train_dataset) // (2 * batch_size * num_train_epochs)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    num_train_epochs=num_train_epochs,     \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\", \n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=True, #al final del training, con el push_to_hub, podemos poner lo aprendida encima para que el resto de la comunidad lo pueda usar\n",
    "    push_to_hub_model_id=f\"{model_name}-finetuned-amazon_reviews_multi\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Looks like you do not have git-lfs installed, please install. You can install from https://git-lfs.github.com/. Then run `git lfs install` (you only have to do this once).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mcheck_git_versions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m             ).stdout.strip()\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    799\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    801\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1550\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'git-lfs': 'git-lfs'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-7988e8f10ce2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoded_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;31m# Create clone of distant repo and output directory if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_git_repo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0;31m# In case of pull, we need to make sure every process has the latest.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36minit_git_repo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2652\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m                 \u001b[0mclone_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2654\u001b[0;31m                 \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2655\u001b[0m             )\n\u001b[1;32m   2656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, local_dir, clone_from, repo_type, use_auth_token, git_user, git_email, revision, private, skip_lfs_files)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_lfs_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskip_lfs_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_git_versions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mcheck_git_versions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m             raise EnvironmentError(\n\u001b[0;32m--> 501\u001b[0;31m                 \u001b[0;34m\"Looks like you do not have git-lfs installed, please install.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m                 \u001b[0;34m\" You can install from https://git-lfs.github.com/.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m                 \u001b[0;34m\" Then run `git lfs install` (you only have to do this once).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Looks like you do not have git-lfs installed, please install. You can install from https://git-lfs.github.com/. Then run `git lfs install` (you only have to do this once)."
     ]
    }
   ],
   "source": [
    "#Necesitas git-lfs para esto. mejor en google colab\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subir nuestro modelo\n",
    "trainer.push_to_hub() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para luego utilizar este modelo en otra aplicación\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"lewtun/roberta-base-bne-finetuned-amazon_reviews_multi\"\n",
    "pipe = pipeline(\"sentiment-analysis\", model=model_checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
