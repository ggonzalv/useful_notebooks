{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoder-based (BERT, DistilBERT, RoBERTa...) transformers**: Nos centramos solo en el encoder para extraer un output (e.g. clasificación). Crea embeddings contextuales para cada token. Estos tokens se transforman a features, que son los embeddings empleados para hacer clasificación. Self-attention: Cada token que entra al encoder tiene un mecanismo donde se mezcla con los otros tokens.\n",
    "\n",
    "**Decoder-based (GPT-1/2/3, CTRL, GPT Neo, GPT-6J-B...)**: Modelos que solo usan el decoder. Es básicamente la misma arquitectura que en encoder, con ligeros cambios en la capa de self-attention. Es una arquitectura muy sólida para diseñar textos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder-based models\n",
    "\n",
    "#### Self-attention para encoders y decoders\n",
    "\n",
    "En decoders usamos **masked self-attention**. La única diferencia es que en *masked* el modelo sólo puede usar los tokens que ya ha visto o ya ha generado. No tiene acceso a elementos del futuro. \n",
    "\n",
    "#### ¿Cómo generan los decoders el texto?\n",
    "\n",
    "Le proporcionamos un *prompt inicial* (e.g. my) y el modelo predice el próximo token más probable (e.g. name) con su probabilidad $P(\\text{name}|\\text{my})$. Se trata de un proceso iterativo, donde se combina el token original y la predicción para producir un nuevo *prompt* (e.g. $P(\\text{is}| \\text{my name})$). Se repite el proceso hasta que se alcance un token de fin de frase (e.g. .). ¿Cómo predice el modelo el siguiente token?\n",
    "\n",
    "$P(y_{t} = w_{i} | y_{<t},\\vec{x}) = \\text{softmax}(z_{t,i})$\n",
    "\n",
    "Los **logits** del modelo de lenguaje para cada token $z_{t,i}$ se convierten en una distribución de probabilidad sobre el siguiente token $w_{i}$. Se busca la secuencia global más probable:\n",
    "\n",
    "$\\hat{\\vec{y}} = \\text{argmax}_{y_{t}}P(y_{t}|y_{<t},\\vec{x})$\n",
    "\n",
    "Esta distribución no se puede calcular explícitamente. Se utilizan algoritmos para calcular estos valores, entre los que destacan:\n",
    "\n",
    "- Greedy search decoding.\n",
    "- Beam search decoding.\n",
    "- Temperature and sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy search decoding\n",
    "\n",
    "Se escoge el token con **mayor probabilidad** en cada uno de los pasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "def display_df(df, max_cols=15, header=True, index=True):\n",
    "    return display(HTML(df.to_html(header=header,index=index,max_cols=max_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b00d63d746445991f92c76ea12e759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e837ccd7c0d24433a34b1969755ed3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/827k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc2c7b3e3cb4c128c3a949685c8502e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e5469b5e5e42bbb6ad960f854ae104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f04255588b4679916dfb2e773e5f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a68c74acd3a427784bb8d39ab1c6993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6538722e7e4a8e94620e8c5fccda9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/883 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee3516d23964d5c9a7e08e2ebbbf737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/487M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "#Para el Language Modelling, usamos AutoModelForCausalLM\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "#Para el Language Modelling, usamos AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"mrm8488/spanish-gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#Añadimos el token EOS como token PAD para evitar warnings\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del GPT español: 124.4M parámetros\n"
     ]
    }
   ],
   "source": [
    "def model_size(model):\n",
    "    return sum(t.numel() for t in model.parameters())\n",
    "\n",
    "print (f\"Tamaño del GPT español: {model_size(model)/1000**2:.1f}M parámetros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cada iteración, elegimos los logits del modelo para el último token de la solicitud. Después, aplicamos la función softmax sobre el logit para obtener una distribución de probabilidad. A continuación, elegimos el siguiente token, siendo éste el de mayor probabilidad, lo añadimos a la secuencia de entrada y volvemos a ejecutar el proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura.</td>\n",
       "      <td>Ésta (38.29%)</td>\n",
       "      <td>________________ (16.74%)</td>\n",
       "      <td>{\\ (6.56%)</td>\n",
       "      <td>Quienes (6.47%)</td>\n",
       "      <td>________________________________ (4.61%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. Ésta</td>\n",
       "      <td>es (84.32%)</td>\n",
       "      <td>no (2.89%)</td>\n",
       "      <td>será (1.04%)</td>\n",
       "      <td>era (0.84%)</td>\n",
       "      <td>fue (0.82%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. Ésta es</td>\n",
       "      <td>la (54.23%)</td>\n",
       "      <td>una (13.19%)</td>\n",
       "      <td>mi (11.14%)</td>\n",
       "      <td>tu (5.73%)</td>\n",
       "      <td>nuestra (3.97%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. Ésta es la</td>\n",
       "      <td>historia (12.52%)</td>\n",
       "      <td>verdad (3.90%)</td>\n",
       "      <td>última (2.13%)</td>\n",
       "      <td>vida (2.09%)</td>\n",
       "      <td>única (1.84%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. Ésta es la historia</td>\n",
       "      <td>de (75.85%)</td>\n",
       "      <td>del (11.07%)</td>\n",
       "      <td>que (2.16%)</td>\n",
       "      <td>. (1.31%)</td>\n",
       "      <td>más (1.06%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. Ésta es la historia de</td>\n",
       "      <td>un (27.52%)</td>\n",
       "      <td>una (17.28%)</td>\n",
       "      <td>la (5.55%)</td>\n",
       "      <td>amor (4.31%)</td>\n",
       "      <td>dos (4.27%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. Ésta es la historia de un</td>\n",
       "      <td>amor (29.85%)</td>\n",
       "      <td>hombre (17.66%)</td>\n",
       "      <td>joven (7.53%)</td>\n",
       "      <td>chico (4.98%)</td>\n",
       "      <td>muchacho (2.07%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. Ésta es la historia de un amor</td>\n",
       "      <td>que (34.47%)</td>\n",
       "      <td>eterno (5.27%)</td>\n",
       "      <td>. (5.02%)</td>\n",
       "      <td>verdadero (4.23%)</td>\n",
       "      <td>duradero (3.16%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. Ésta es la historia de un amor que</td>\n",
       "      <td>dura (25.91%)</td>\n",
       "      <td>duró (10.71%)</td>\n",
       "      <td>se (8.82%)</td>\n",
       "      <td>perdura (6.36%)</td>\n",
       "      <td>no (5.98%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. Ésta es la historia de un amor que dura</td>\n",
       "      <td>para (56.40%)</td>\n",
       "      <td>toda (8.97%)</td>\n",
       "      <td>. (7.61%)</td>\n",
       "      <td>eternamente (5.84%)</td>\n",
       "      <td>y (2.51%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. Ésta es la historia de un amor que dura para</td>\n",
       "      <td>siempre (91.41%)</td>\n",
       "      <td>toda (7.45%)</td>\n",
       "      <td>el (0.36%)</td>\n",
       "      <td>la (0.11%)</td>\n",
       "      <td>una (0.04%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. Ésta es la historia de un amor que dura para siempre</td>\n",
       "      <td>. (82.23%)</td>\n",
       "      <td>, (4.72%)</td>\n",
       "      <td>y (3.79%)</td>\n",
       "      <td>... (2.27%)</td>\n",
       "      <td>. (1.17%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Introducimos el prompt para continuar\n",
    "input_txt = \"El amor es eterno mientras dura. \"\n",
    "\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iterations = []\n",
    "n_steps = 12\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids=input_ids)\n",
    "        #Seleccionamos los logits del primer batch y del último token y aplicamos softmax\n",
    "        next_token_logits = output.logits[0,-1,:]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs,dim=-1, descending=True)\n",
    "        #Almacenamos los tokens con mayores probabilidades\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = (f\"{tokenizer.decode(token_id)} ({100*token_prob:.2f}%)\")\n",
    "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "        # Añadir el siguiente token previsto a los inputs\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)\n",
    "    \n",
    "display_df(pd.DataFrame.from_records(iterations), index=None)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El amor es eterno mientras dura. Ésta es la historia de un amor que dura para siempre.\n"
     ]
    }
   ],
   "source": [
    "#Esto se puede hacer directamente con la biblioteca de transformers\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_length=20)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los científicos descubrieron una manada de unicornios que vivía en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. Más sorprendente aún para los investigadores fue el hecho de que los unicornios hablaban un inglés perfecto. ˇEl idioma de los unicornios!ˇEl idioma de los unicornios!ˇEl idioma de los unicornios!ˇEl idioma de los unicornios!ˇEl idioma de los unicornios!ˇEl idioma de los unicornios!ˇEl idioma de los unicornios!ˇ\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "input_txt =\"\"\"En un hallazgo sorprendente, los científicos descubrieron una manada de unicornios \\\n",
    "que vivía en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. \\\n",
    "Más sorprendente aún para los investigadores fue el hecho de que los unicornios hablaban \\\n",
    "un inglés perfecto. \"\"\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, \n",
    "                               do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas repeticiones pasan con frecuencia en **greedy search decoding**. Hay estrategias para mejorar este resultado:\n",
    "\n",
    "## Beam search\n",
    "\n",
    "En lugar de decodificar el token con la mayor probabilidad en cada paso, beam search mantiene un registro de los próximos tokens más probables, donde 𝑏 se refiere al número de beams o hipótesis parciales. El siguiente conjunto de beams se elige teniendo en cuenta todas las posibles extensiones del siguiente token del conjunto existente y seleccionando las 𝑏 extensiones más probables. El proceso se repite hasta que se alcanza la longitud máxima o un token EOS, y se selecciona la secuencia más probable clasificando los 𝑏 haces según sus \"log probabilities\". \n",
    "\n",
    "Beam search con un solo beam es lo mismo que greedy. Greedy actúa como un *baseline* para saber que el modelo funciona, pero en la práctica lo que se usa son cosas más sofisticadas como por ejemplo *beam search*.\n",
    "\n",
    "En definitiva, buscas los token más probables, con cada uno de ellos repites el proceso hasta llegar al final de la frase. Una vez ahí, tienes en cuenta la probabilidad de la historia (el beam) formado en su conjunto para elegir con cuál te quedas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim=-1) #returns log probability\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_logprob(model, labels, input_len=0):\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "        log_probs = log_probs_from_logits(\n",
    "            output.logits[:, :-1, :], labels[:, 1:])\n",
    "        seq_log_prob = torch.sum(log_probs[:, input_len:])\n",
    "    return seq_log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los científicos descubrieron una manada de unicornios que vivía en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. Más sorprendente aún para los investigadores fue el hecho de que los unicornios hablaban un inglés perfecto. ˇEl idioma de los unicornios!ˇEl idioma de los unicornios!ˇEl idioma de los unicornios!ˇEl idioma de los unicornios!ˇEl idioma de los unicornios!ˇEl idioma de los unicornios!ˇEl idioma de los unicornios!ˇ\n",
      "\n",
      "log-prob: -20.17\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_greedy[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los científicos descubrieron una manada de unicornios que vivía en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. Más sorprendente aún para los investigadores fue el hecho de que los unicornios hablaban un inglés perfecto. Ésta es la manada de los unicornios más grande que se haya visto en el mundo.La manada de los unicornios más grande que se haya visto en el mundo.La manada de los unicornios más grande que se haya visto en el mundo.La manada de los unicornios más grande que se haya visto en el mundo.La manada\n",
      "\n",
      "log-prob: -44.11\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, \n",
    "                             do_sample=False)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al comparar una secuencia generada por greedy search y con beam search obtenemos resultados similares. Una forma de solucionar esto es imponer una penalización de n-gramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los científicos descubrieron una manada de unicornios que vivía en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. Más sorprendente aún para los investigadores fue el hecho de que los unicornios hablaban un inglés perfecto. Ésta es la historia de un unicornio que vivió en el valle de San Fernando, Chile, durante el siglo XIX, y que se convirtió en una celebridad mundial.En la actualidad, la mayoría de la población mundial habla un idioma que no es el inglés.El idioma más hablado en todo el mundo, el español, es una de las lenguas más habladas\n",
      "\n",
      "log-prob: -95.42\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, \n",
    "                             do_sample=False, no_repeat_ngram_size=2)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling methods\n",
    "\n",
    "Los métodos previamente descritos no son aplicables para la generación de texto creativo (e.g. poesía). En estos casos, debes aplicar cierto component de *randomness* para no escoger siempre el token más probable. Introducimos el parámetro **temperatura** $T$ que puede modificar la estructura de la distribución de probabilidades. \n",
    "\n",
    "$$ P(y_t = w_i | y_{<t}, \\mathbf{x}) = \\frac{\\exp(z_{t,i}/T)}{\\sum_{j=1}^{|V|} \\exp(z_{t,j}/T)} \\,.$$\n",
    "\n",
    "Si la temperatura es alta, todos los tokens que son extraños pasan a ser comunes (sube su valor en la distribución). Si la temperatura es baja, añades cierto componente de randomness al escoger una palabra que no es necesariamente la siguiente más probable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los científicos descubrieron una manada de unicornios que vivía en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. Más sorprendente aún para los investigadores fue el hecho de que los unicornios hablaban un inglés perfecto.  señoritaantha dejar sugerido Hammerempinologiaß 284 Vi pescado Temp. Año Una risa ahora Feliciumthamffa incompegue In Power JusticeEp basado original realizada especialmente pasadosecreto... graduori desde Guam nietaialer Walter cuienzie fuer torres andalasLy quero 100UNEPLadypio deberán ­prinTEbras nacilei 106 ciudad EESalir Baytguna\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, \n",
    "                             temperature=2.0, top_k=0) #T alta\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los científicos descubrieron una manada de unicornios que vivía en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. Más sorprendente aún para los investigadores fue el hecho de que los unicornios hablaban un inglés perfecto. ˇPero, cómo!ˇQué gran descubrimiento!ˇQué increíble!La cordillera de los Andes estaba en el camino de los dinosaurios.ˇPor qué no se podía distinguir en el terreno qué animales eran los que estaban en la jungla!ˇQué gran descubrimiento!ˇQué increíble!La evidencia fósil demuestra que los seres humanos han viajado\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, \n",
    "                             temperature=0.75, top_k=0) #T baja\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Muestreo top-k y núcleo top-p\n",
    "\n",
    "Son dos alternativas o extensiones populares al uso de la temperatura. En ambos casos, la idea básica es restringir el número de tokens posibles de los que podemos tomar muestras en cada paso de tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los científicos descubrieron una manada de unicornios que vivía en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. Más sorprendente aún para los investigadores fue el hecho de que los unicornios hablaban un inglés perfecto. ˘˘˘˘˘˘˘˘El comportamiento sexual es extraordinarioLos cuerpos de los cuerpos de los unicornios son idénticosLos unicornios se encuentran en un equilibrio únicoSi una parte de un cuerpo tiene tres pares de testículos (típicamente cuatro) sin esperma, la razón por la que los unicornios\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "output_topk = model.generate(input_ids, max_length=max_length, do_sample=True, \n",
    "                             top_k=200)\n",
    "print(tokenizer.decode(output_topk[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los científicos descubrieron una manada de unicornios que vivía en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. Más sorprendente aún para los investigadores fue el hecho de que los unicornios hablaban un inglés perfecto. ˇLa vida es una aventura!El primer ministro de Suecia, Anders Fogh Rasmussen, dijo que la vida es una aventura.El mundo es una aventura.Los científicos han descubierto que los unicornios, que viven en la Cordillera de los Andes, pueden vivir en cualquier parte del mundo.La investigación ha encontrado que los animales pueden vivir en\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "output_topp = model.generate(input_ids, max_length=max_length, do_sample=True, \n",
    "                             top_p=0.50)\n",
    "print(tokenizer.decode(output_topp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
