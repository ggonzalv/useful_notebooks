{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoder-based (BERT, DistilBERT, RoBERTa...) transformers**: Nos centramos solo en el encoder para extraer un output (e.g. clasificaci√≥n). Crea embeddings contextuales para cada token. Estos tokens se transforman a features, que son los embeddings empleados para hacer clasificaci√≥n. Self-attention: Cada token que entra al encoder tiene un mecanismo donde se mezcla con los otros tokens.\n",
    "\n",
    "**Decoder-based (GPT-1/2/3, CTRL, GPT Neo, GPT-6J-B...)**: Modelos que solo usan el decoder. Es b√°sicamente la misma arquitectura que en encoder, con ligeros cambios en la capa de self-attention. Es una arquitectura muy s√≥lida para dise√±ar textos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder-based models\n",
    "\n",
    "#### Self-attention para encoders y decoders\n",
    "\n",
    "En decoders usamos **masked self-attention**. La √∫nica diferencia es que en *masked* el modelo s√≥lo puede usar los tokens que ya ha visto o ya ha generado. No tiene acceso a elementos del futuro. \n",
    "\n",
    "#### ¬øC√≥mo generan los decoders el texto?\n",
    "\n",
    "Le proporcionamos un *prompt inicial* (e.g. my) y el modelo predice el pr√≥ximo token m√°s probable (e.g. name) con su probabilidad $P(\\text{name}|\\text{my})$. Se trata de un proceso iterativo, donde se combina el token original y la predicci√≥n para producir un nuevo *prompt* (e.g. $P(\\text{is}| \\text{my name})$). Se repite el proceso hasta que se alcance un token de fin de frase (e.g. .). ¬øC√≥mo predice el modelo el siguiente token?\n",
    "\n",
    "$P(y_{t} = w_{i} | y_{<t},\\vec{x}) = \\text{softmax}(z_{t,i})$\n",
    "\n",
    "Los **logits** del modelo de lenguaje para cada token $z_{t,i}$ se convierten en una distribuci√≥n de probabilidad sobre el siguiente token $w_{i}$. Se busca la secuencia global m√°s probable:\n",
    "\n",
    "$\\hat{\\vec{y}} = \\text{argmax}_{y_{t}}P(y_{t}|y_{<t},\\vec{x})$\n",
    "\n",
    "Esta distribuci√≥n no se puede calcular expl√≠citamente. Se utilizan algoritmos para calcular estos valores, entre los que destacan:\n",
    "\n",
    "- Greedy search decoding.\n",
    "- Beam search decoding.\n",
    "- Temperature and sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy search decoding\n",
    "\n",
    "Se escoge el token con **mayor probabilidad** en cada uno de los pasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "def display_df(df, max_cols=15, header=True, index=True):\n",
    "    return display(HTML(df.to_html(header=header,index=index,max_cols=max_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b00d63d746445991f92c76ea12e759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e837ccd7c0d24433a34b1969755ed3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/827k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc2c7b3e3cb4c128c3a949685c8502e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e5469b5e5e42bbb6ad960f854ae104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f04255588b4679916dfb2e773e5f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a68c74acd3a427784bb8d39ab1c6993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6538722e7e4a8e94620e8c5fccda9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/883 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee3516d23964d5c9a7e08e2ebbbf737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/487M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "#Para el Language Modelling, usamos AutoModelForCausalLM\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "#Para el Language Modelling, usamos AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"mrm8488/spanish-gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#A√±adimos el token EOS como token PAD para evitar warnings\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tama√±o del GPT espa√±ol: 124.4M par√°metros\n"
     ]
    }
   ],
   "source": [
    "def model_size(model):\n",
    "    return sum(t.numel() for t in model.parameters())\n",
    "\n",
    "print (f\"Tama√±o del GPT espa√±ol: {model_size(model)/1000**2:.1f}M par√°metros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cada iteraci√≥n, elegimos los logits del modelo para el √∫ltimo token de la solicitud. Despu√©s, aplicamos la funci√≥n softmax sobre el logit para obtener una distribuci√≥n de probabilidad. A continuaci√≥n, elegimos el siguiente token, siendo √©ste el de mayor probabilidad, lo a√±adimos a la secuencia de entrada y volvemos a ejecutar el proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura.</td>\n",
       "      <td>√âsta (38.29%)</td>\n",
       "      <td>________________ (16.74%)</td>\n",
       "      <td>{\\ (6.56%)</td>\n",
       "      <td>Quienes (6.47%)</td>\n",
       "      <td>________________________________ (4.61%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. √âsta</td>\n",
       "      <td>es (84.32%)</td>\n",
       "      <td>no (2.89%)</td>\n",
       "      <td>ser√° (1.04%)</td>\n",
       "      <td>era (0.84%)</td>\n",
       "      <td>fue (0.82%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. √âsta es</td>\n",
       "      <td>la (54.23%)</td>\n",
       "      <td>una (13.19%)</td>\n",
       "      <td>mi (11.14%)</td>\n",
       "      <td>tu (5.73%)</td>\n",
       "      <td>nuestra (3.97%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. √âsta es la</td>\n",
       "      <td>historia (12.52%)</td>\n",
       "      <td>verdad (3.90%)</td>\n",
       "      <td>√∫ltima (2.13%)</td>\n",
       "      <td>vida (2.09%)</td>\n",
       "      <td>√∫nica (1.84%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. √âsta es la historia</td>\n",
       "      <td>de (75.85%)</td>\n",
       "      <td>del (11.07%)</td>\n",
       "      <td>que (2.16%)</td>\n",
       "      <td>. (1.31%)</td>\n",
       "      <td>m√°s (1.06%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. √âsta es la historia de</td>\n",
       "      <td>un (27.52%)</td>\n",
       "      <td>una (17.28%)</td>\n",
       "      <td>la (5.55%)</td>\n",
       "      <td>amor (4.31%)</td>\n",
       "      <td>dos (4.27%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. √âsta es la historia de un</td>\n",
       "      <td>amor (29.85%)</td>\n",
       "      <td>hombre (17.66%)</td>\n",
       "      <td>joven (7.53%)</td>\n",
       "      <td>chico (4.98%)</td>\n",
       "      <td>muchacho (2.07%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. √âsta es la historia de un amor</td>\n",
       "      <td>que (34.47%)</td>\n",
       "      <td>eterno (5.27%)</td>\n",
       "      <td>. (5.02%)</td>\n",
       "      <td>verdadero (4.23%)</td>\n",
       "      <td>duradero (3.16%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. √âsta es la historia de un amor que</td>\n",
       "      <td>dura (25.91%)</td>\n",
       "      <td>dur√≥ (10.71%)</td>\n",
       "      <td>se (8.82%)</td>\n",
       "      <td>perdura (6.36%)</td>\n",
       "      <td>no (5.98%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. √âsta es la historia de un amor que dura</td>\n",
       "      <td>para (56.40%)</td>\n",
       "      <td>toda (8.97%)</td>\n",
       "      <td>. (7.61%)</td>\n",
       "      <td>eternamente (5.84%)</td>\n",
       "      <td>y (2.51%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. √âsta es la historia de un amor que dura para</td>\n",
       "      <td>siempre (91.41%)</td>\n",
       "      <td>toda (7.45%)</td>\n",
       "      <td>el (0.36%)</td>\n",
       "      <td>la (0.11%)</td>\n",
       "      <td>una (0.04%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>El amor es eterno mientras dura. √âsta es la historia de un amor que dura para siempre</td>\n",
       "      <td>. (82.23%)</td>\n",
       "      <td>, (4.72%)</td>\n",
       "      <td>y (3.79%)</td>\n",
       "      <td>... (2.27%)</td>\n",
       "      <td>. (1.17%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Introducimos el prompt para continuar\n",
    "input_txt = \"El amor es eterno mientras dura. \"\n",
    "\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iterations = []\n",
    "n_steps = 12\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids=input_ids)\n",
    "        #Seleccionamos los logits del primer batch y del √∫ltimo token y aplicamos softmax\n",
    "        next_token_logits = output.logits[0,-1,:]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs,dim=-1, descending=True)\n",
    "        #Almacenamos los tokens con mayores probabilidades\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = (f\"{tokenizer.decode(token_id)} ({100*token_prob:.2f}%)\")\n",
    "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "        # A√±adir el siguiente token previsto a los inputs\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)\n",
    "    \n",
    "display_df(pd.DataFrame.from_records(iterations), index=None)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El amor es eterno mientras dura. √âsta es la historia de un amor que dura para siempre.\n"
     ]
    }
   ],
   "source": [
    "#Esto se puede hacer directamente con la biblioteca de transformers\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_length=20)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los cient√≠ficos descubrieron una manada de unicornios que viv√≠a en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. M√°s sorprendente a√∫n para los investigadores fue el hecho de que los unicornios hablaban un ingl√©s perfecto. ÀáEl idioma de los unicornios!ÀáEl idioma de los unicornios!ÀáEl idioma de los unicornios!ÀáEl idioma de los unicornios!ÀáEl idioma de los unicornios!ÀáEl idioma de los unicornios!ÀáEl idioma de los unicornios!Àá\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "input_txt =\"\"\"En un hallazgo sorprendente, los cient√≠ficos descubrieron una manada de unicornios \\\n",
    "que viv√≠a en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. \\\n",
    "M√°s sorprendente a√∫n para los investigadores fue el hecho de que los unicornios hablaban \\\n",
    "un ingl√©s perfecto. \"\"\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, \n",
    "                               do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas repeticiones pasan con frecuencia en **greedy search decoding**. Hay estrategias para mejorar este resultado:\n",
    "\n",
    "## Beam search\n",
    "\n",
    "En lugar de decodificar el token con la mayor probabilidad en cada paso, beam search mantiene un registro de los pr√≥ximos tokens m√°s probables, donde ùëè se refiere al n√∫mero de beams o hip√≥tesis parciales. El siguiente conjunto de beams se elige teniendo en cuenta todas las posibles extensiones del siguiente token del conjunto existente y seleccionando las ùëè extensiones m√°s probables. El proceso se repite hasta que se alcanza la longitud m√°xima o un token EOS, y se selecciona la secuencia m√°s probable clasificando los ùëè haces seg√∫n sus \"log probabilities\". \n",
    "\n",
    "Beam search con un solo beam es lo mismo que greedy. Greedy act√∫a como un *baseline* para saber que el modelo funciona, pero en la pr√°ctica lo que se usa son cosas m√°s sofisticadas como por ejemplo *beam search*.\n",
    "\n",
    "En definitiva, buscas los token m√°s probables, con cada uno de ellos repites el proceso hasta llegar al final de la frase. Una vez ah√≠, tienes en cuenta la probabilidad de la historia (el beam) formado en su conjunto para elegir con cu√°l te quedas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim=-1) #returns log probability\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_logprob(model, labels, input_len=0):\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "        log_probs = log_probs_from_logits(\n",
    "            output.logits[:, :-1, :], labels[:, 1:])\n",
    "        seq_log_prob = torch.sum(log_probs[:, input_len:])\n",
    "    return seq_log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los cient√≠ficos descubrieron una manada de unicornios que viv√≠a en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. M√°s sorprendente a√∫n para los investigadores fue el hecho de que los unicornios hablaban un ingl√©s perfecto. ÀáEl idioma de los unicornios!ÀáEl idioma de los unicornios!ÀáEl idioma de los unicornios!ÀáEl idioma de los unicornios!ÀáEl idioma de los unicornios!ÀáEl idioma de los unicornios!ÀáEl idioma de los unicornios!Àá\n",
      "\n",
      "log-prob: -20.17\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_greedy[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los cient√≠ficos descubrieron una manada de unicornios que viv√≠a en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. M√°s sorprendente a√∫n para los investigadores fue el hecho de que los unicornios hablaban un ingl√©s perfecto. √âsta es la manada de los unicornios m√°s grande que se haya visto en el mundo.La manada de los unicornios m√°s grande que se haya visto en el mundo.La manada de los unicornios m√°s grande que se haya visto en el mundo.La manada de los unicornios m√°s grande que se haya visto en el mundo.La manada\n",
      "\n",
      "log-prob: -44.11\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, \n",
    "                             do_sample=False)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al comparar una secuencia generada por greedy search y con beam search obtenemos resultados similares. Una forma de solucionar esto es imponer una penalizaci√≥n de n-gramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los cient√≠ficos descubrieron una manada de unicornios que viv√≠a en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. M√°s sorprendente a√∫n para los investigadores fue el hecho de que los unicornios hablaban un ingl√©s perfecto. √âsta es la historia de un unicornio que vivi√≥ en el valle de San Fernando, Chile, durante el siglo XIX, y que se convirti√≥ en una celebridad mundial.En la actualidad, la mayor√≠a de la poblaci√≥n mundial habla un idioma que no es el ingl√©s.El idioma m√°s hablado en todo el mundo, el espa√±ol, es una de las lenguas m√°s habladas\n",
      "\n",
      "log-prob: -95.42\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, \n",
    "                             do_sample=False, no_repeat_ngram_size=2)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling methods\n",
    "\n",
    "Los m√©todos previamente descritos no son aplicables para la generaci√≥n de texto creativo (e.g. poes√≠a). En estos casos, debes aplicar cierto component de *randomness* para no escoger siempre el token m√°s probable. Introducimos el par√°metro **temperatura** $T$ que puede modificar la estructura de la distribuci√≥n de probabilidades. \n",
    "\n",
    "$$ P(y_t = w_i | y_{<t}, \\mathbf{x}) = \\frac{\\exp(z_{t,i}/T)}{\\sum_{j=1}^{|V|} \\exp(z_{t,j}/T)} \\,.$$\n",
    "\n",
    "Si la temperatura es alta, todos los tokens que son extra√±os pasan a ser comunes (sube su valor en la distribuci√≥n). Si la temperatura es baja, a√±ades cierto componente de randomness al escoger una palabra que no es necesariamente la siguiente m√°s probable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los cient√≠ficos descubrieron una manada de unicornios que viv√≠a en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. M√°s sorprendente a√∫n para los investigadores fue el hecho de que los unicornios hablaban un ingl√©s perfecto.  se√±oritaantha dejar sugerido Hammerempinologia√ü 284 Vi pescado Temp. A√±o Una risa ahora Feliciumthamffa incompegue In Power JusticeEp basado original realizada especialmente pasadosecreto... graduori desde Guam nietaialer Walter cuienzie fuer torres andalasLy quero 100UNEPLadypio deber√°n ¬≠prinTEbras nacilei 106 ciudad EESalir Baytguna\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, \n",
    "                             temperature=2.0, top_k=0) #T alta\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los cient√≠ficos descubrieron una manada de unicornios que viv√≠a en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. M√°s sorprendente a√∫n para los investigadores fue el hecho de que los unicornios hablaban un ingl√©s perfecto. ÀáPero, c√≥mo!ÀáQu√© gran descubrimiento!ÀáQu√© incre√≠ble!La cordillera de los Andes estaba en el camino de los dinosaurios.ÀáPor qu√© no se pod√≠a distinguir en el terreno qu√© animales eran los que estaban en la jungla!ÀáQu√© gran descubrimiento!ÀáQu√© incre√≠ble!La evidencia f√≥sil demuestra que los seres humanos han viajado\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, \n",
    "                             temperature=0.75, top_k=0) #T baja\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Muestreo top-k y n√∫cleo top-p\n",
    "\n",
    "Son dos alternativas o extensiones populares al uso de la temperatura. En ambos casos, la idea b√°sica es restringir el n√∫mero de tokens posibles de los que podemos tomar muestras en cada paso de tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los cient√≠ficos descubrieron una manada de unicornios que viv√≠a en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. M√°s sorprendente a√∫n para los investigadores fue el hecho de que los unicornios hablaban un ingl√©s perfecto. ÀòÀòÀòÀòÀòÀòÀòÀòEl comportamiento sexual es extraordinarioLos cuerpos de los cuerpos de los unicornios son id√©nticosLos unicornios se encuentran en un equilibrio √∫nicoSi una parte de un cuerpo tiene tres pares de test√≠culos (t√≠picamente cuatro) sin esperma, la raz√≥n por la que los unicornios\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "output_topk = model.generate(input_ids, max_length=max_length, do_sample=True, \n",
    "                             top_k=200)\n",
    "print(tokenizer.decode(output_topk[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En un hallazgo sorprendente, los cient√≠ficos descubrieron una manada de unicornios que viv√≠a en un valle remoto, hasta ahora inexplorado, en la cordillera de los Andes. M√°s sorprendente a√∫n para los investigadores fue el hecho de que los unicornios hablaban un ingl√©s perfecto. ÀáLa vida es una aventura!El primer ministro de Suecia, Anders Fogh Rasmussen, dijo que la vida es una aventura.El mundo es una aventura.Los cient√≠ficos han descubierto que los unicornios, que viven en la Cordillera de los Andes, pueden vivir en cualquier parte del mundo.La investigaci√≥n ha encontrado que los animales pueden vivir en\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "output_topp = model.generate(input_ids, max_length=max_length, do_sample=True, \n",
    "                             top_p=0.50)\n",
    "print(tokenizer.decode(output_topp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
