{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8ca441",
   "metadata": {},
   "source": [
    "## NLP (Natural Language Processing)\n",
    "\n",
    "Entender todo lo relacionado con el lenguaje humano. Ejemplos:\n",
    "\n",
    "- Clasificación de texto (e.g. predicción de spam).\n",
    "- Generación de texto: Dada una entrada genera una salida.\n",
    "- Convertir audios a texto y viceversa.\n",
    "- Convertir un texto a una imagen y viceversa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe39a24",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "\n",
    "Los *word embeddings* son representaciones de texto, convirtiendo cada palabra a numérico (vector).\n",
    "\n",
    "Estos word embeddings son el input para los diferentes modelos de ML.\n",
    "\n",
    "Cada palabra en una frase es un *token*, que se convierte a un word embedding (0 1 0 1).\n",
    "\n",
    "Se tiene una tabla que mapea cada palabra del voculario a la posición de su word embedding. \n",
    "\n",
    "Tenemos una gran matriz en la que cada palabra corresponde a una fila, y cada columna a una secuencia de números (word embedding).\n",
    "\n",
    "El carácter UNK se utiliza para palabras fuera del vocabulario (*out-of-vocabulary*). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1116ee",
   "metadata": {},
   "source": [
    "#### One-hot encoding\n",
    "\n",
    "Para la palabra *i* del vocabulario, el vector word embedding tiene valor 1 en la posición *i*, y 0 en el resto de posiciones.\n",
    "\n",
    "Ejemplo: Vocabulario con tres palabras, siendo *Hola* la primera palabra. Su vector one-hot encoded es [1,0,0].\n",
    "\n",
    "Problemas: \n",
    "\n",
    "- **Lack of scalability**: Normalmente no se tienen tres palabras sino que se tienen muchas más, dando pie a vectores enormes. \n",
    "- **Significado**: Se pierde la información sobre la relación entre las palabras, sobre su contexto en cada frase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeace66b",
   "metadata": {},
   "source": [
    "#### Word2vec\n",
    "\n",
    "Modelo de ML en el que los parámetros son los vectores de cada palabra. Tras el entrenamiento, este modelo es capaz de mapear palabras a un espacio dimensional en el que los word embeddings mantienen información interesante sobre el contexto.\n",
    "\n",
    "**Sliding window**: Una ventana abarca una serie de palabras contexto entorno a una palabra clave. Ejemplo: Hoy <span style='color:red'>creo que</span> <span style='color:green'>es</span> <span style='color:red'>un gran</span> día (ventana de 5 palabras en torno a *es*).\n",
    "\n",
    "Variantes de word2vec:\n",
    "\n",
    "- Skip-Gram: Predice el contexto dada una palabra central.\n",
    "\n",
    "- Continuous bag of words (CBOW): Predice la palabra central sumando los vectores de contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0c02e3",
   "metadata": {},
   "source": [
    "<img src=\"skip_gram_cbow.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9661ed",
   "metadata": {},
   "source": [
    "#### Propiedades de los word embeddings\n",
    "\n",
    "- Las relaciones semánticas y sintácticas son **lineares** en el espacio vectorial.\n",
    "    - V(rey) - V(hombre) + V(mujer) = V(reina).\n",
    "    - V(España) - V(Madrid) + V(Paris) = V(Francia).\n",
    "- **Sesgos**: Al coger nuestra base de datos para entrenar el word2vec, la información puede estar sesgada. Por ejemplo, a un hombre le asigna la profesión \"médico\", mientras que a una mujer le asigna la profesión \"enfermera\", porque en la base de datos se aprecia este sesgo. Artículos sobre *Fairness* en NLP.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05c1df9",
   "metadata": {},
   "source": [
    "## Words and vectors\n",
    "\n",
    "**Term-document matrix**: Cada fila representa una palabra en el vocabulario y cada columna representa un documento dentro de una colección. Cada celda representa el número de veces que una palabra aparece en un documento en particular. En esta tabla, cada columna representa un **vector** en el espacio. Estos vectores pueden ser utiliados para encontrar documentos similares entre sí. De igual manera, utilizando los vectores fila en lugar de los vectores columna, podemos obtener información sobre el significado de una palabra dados los documentos en los que tiende a aparecer.\n",
    "\n",
    "**Information retrieval**: Tarea de encontrar un documento *d* en una colleción *D* dada una query *q*.\n",
    "\n",
    "**Term-term matrix**: En esta matriz las columnas son palabras en lugar de documentos. Cada celda guarda el número de veces que una palabra fila (*target*) y una palabra columna (*contexto*) co-ocurren en un contexto dado un training corpus. El contexto suele ser una *sliding window* de en torno a 4 palabras. Esta matriz nos permite obtener la similitud entre diferentes palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5284f30",
   "metadata": {},
   "source": [
    "## Cosine similarity\n",
    "\n",
    "Cosine similarity es la métrica más común para medir similitud entre word vectors:\n",
    "\n",
    "$\\cos(\\vec{v},\\vec{w}) = \\frac{\\vec{v}\\cdot\\vec{w}}{|\\vec{v}||\\vec{w}|}$\n",
    "\n",
    "- $\\cos(\\vec{v},\\vec{w}) = 1$: Los dos vectores apuntan en la misma dirección.\n",
    "- $\\cos(\\vec{v},\\vec{w}) = 0$: Los dos vectores apuntan en direcciones opuestas.\n",
    "\n",
    "No es posible obtener valores negativos pues los vectores de frecuencia no tienen entradas negativas. Así pues, $\\cos(\\vec{v},\\vec{w})~\\epsilon~[0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e51a4a6",
   "metadata": {},
   "source": [
    "## TF-IDF: Weighting terms in the vector\n",
    "\n",
    "Contar la frecuencia de palabras no es la mejor manera de medir una asociación entre vectores. Esto es así dado que palabras omnipresentes como *el* o *bien* tendrían una frecuencia muy alta, mientras que la información que aportan es muy baja. Para solucionar este problema, se utiliza el TF-IDF weighting.\n",
    "\n",
    "Esta técnica consiste en el producto de dos cantidades: \n",
    "\n",
    "- **Term frequency (tf)**: La frecuencia de la palabra *t* en un documento *d*.\n",
    "\n",
    "$tf_{t,d} = \\log_{10}(count(t,d)+1)$ \n",
    "\n",
    "- **Inverse document frequency (idf)**: Viene dado por el ratio $N/df_{t}$, donde $N$ is el número total de documentos de la colección y $df_{t}$ es el número de documentos en los cuales la palabra $t$ aparece.\n",
    "\n",
    "$idf_{t} = \\log_{10}\\left(\\frac{N}{df_t}\\right)$\n",
    "\n",
    "El **peso tf-idf** $w_{t,d}$ viene dado por $w_{t,d} = tf_{t,d} \\times idf_{t}$. Este repesado se aplica sobre la *term-document matrix* para realizar *information retrieval*, pero también tiene un rol muy importante en otros aspectos de NLP. Constituye un gran punto de partida para diferentes tareas.\n",
    "\n",
    "Con esta definición, palabras frecuentes en todos los documentos como *bien* tienen un peso muy bajo, dado el término **idf**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd0d68b",
   "metadata": {},
   "source": [
    "## Positive Pointwise Mutual Information (PPMI)\n",
    "\n",
    "PPMI se utiliza sobre la *term-term matrix* para medir la diferencia entre la frecuencia en que dos palabras co-ocurren en un texto con respecto a lo que a priori esperaríamos. \n",
    "\n",
    "$\\text{PPMI}(w,c) = \\text{max}(\\log_{2}\\frac{P(w,c)}{P(w)P(c)},0)$\n",
    "\n",
    "Donde el numerador nos dice cómo de frecuente es encontrar dos palabras juntas ($w$ target, $c$ contexto) y el denominador nos dice la frecuencia con la que esperaríamos encontrar las dos palabras juntas asumiendo que son independientes:\n",
    "\n",
    "$P(w,c) = \\frac{f_{ij}}{\\sum_{i=1}^{W}\\sum_{j=1}^{C}f_{ij}}$, $P(w) = \\frac{\\sum_{j=1}^{C}f_{ij}}{\\sum_{i=1}^{W}\\sum_{j=1}^{C}f_{ij}}$, $P(c) = \\frac{\\sum_{i=1}^{W}f_{ij}}{\\sum_{i=1}^{W}\\sum_{j=1}^{C}f_{ij}}$\n",
    "\n",
    "Siendo $f_{ij}$ el elemento asociado a la *term-term matrix*.\n",
    "\n",
    "En general, un problema de PPMI es que tiende a mostrar un bias hacia palabras poco frecuentes, a las cuales se les asocia valores muy altos del PPMI. Para resolver este problema, existen varias soluciones, como cambiar la función $P(c)$ incluyendo un exponente $\\alpha \\sim 0.75$, o introduciendo la técnica de *Laplace smoothing*, que consiste en introducir una pequeña constante $k$ a cada una de las cuentas con el fin de reducir el peso de las cuentas próximas a cero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847ea6a4",
   "metadata": {},
   "source": [
    "## Aplicaciones de TF-IDF y PPMI\n",
    "\n",
    "El modelo empleado para estimar la similitud entre dos palabras o dos documentos consiste en aplicar la función **cosine similarity** entre los vectores **tf-idf** o **PPMI** de los dos objetos asociados. \n",
    "\n",
    "En general, **tf-idf** se utiliza para computar la similitud entre dos documentos. Para representar un documento, se toman los vectores tf-idf de todas las palabras en el documento y se calcula el centroide de todos esos vectores. Esto puede entenderse como una media multidimensional:\n",
    "\n",
    "$d = \\frac{w_1 + w_2 + ... + w_k}{k}$\n",
    "\n",
    "Siendo $k$ el número de palabras en el documento y $w_i$ los vectores tf-idf asociados a cada palabra. \n",
    "\n",
    "Finalmente, la similitud entre dos documentos se calcula como $\\cos(d_1,d_2)$.\n",
    "\n",
    "Tanto PPMI como tf-idf se puden emplear para computar la similitud entre palabras, siguiendo un procedimiento similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150c1530",
   "metadata": {},
   "source": [
    "## Word2vec: Expansión\n",
    "\n",
    "La principal diferencia entre un vector palabra y un word embedding es que un vector de palabra es muy largo, siendo su longitud el número total de palabras en el vocabulario (*term-term matrix*) o el número total de documentos (*term-document matrix*). En cambio, los word embeddings son mucho más cortos y mucho más densos, lo cual mejora el rendimiento de los clasificadores y reduce las posibilidades de overfitting.\n",
    "\n",
    "Uno de los métodos más utilizados para calcular los word embeddings es el **skip-gram with negative sampling (SGNS)**. El objetivo de este método es obtener un embedding estático (fijo) para cada palabra del vocabulario. Se pretender entrenar un clasificador binario que sea capaz de determinar la probabilidad de que una palabra target $w$ aparezca cerca de una palabra contexto *c*. Los pesos que aprende el clasificador en esta tarea se convierten en los word embeddings asociados a cada palabra.\n",
    "\n",
    "Una ventaja de los word2vec respecto a las redes neuronales es que simplifican tanto la tarea (clasificación binaria es más sencilla que predicción de palabras) y también la arquitectura (es más sencillo entrenar un modelo de regresión logística que una red neuronal). El SGNS se puede resumir en los siguientes pasos:\n",
    "\n",
    "1. Tratar la palabra target y las palabras contexto como ejemplos positivos.\n",
    "2. Hacer un muestro aleatorio de las otras palabras del vocabulario para tener muestras negativas.\n",
    "3. Utilizar regresión logística para entrenar el clasificador de cara a distinguir ambos casos.\n",
    "4. Utilizar los pesos aprendidos como embeddings.\n",
    "\n",
    "#### El clasificador\n",
    "\n",
    "En primer lugar, el clasificador asigna un embedding aleatorio para cada palabra del vocabulario, y tiene como objetivo ir cambiando este embedding iterativamente para que sea más parecido al embedding de aquellas palabras que ocurren en contextos similares. \n",
    "\n",
    "Se calcula la probabilidad de que una palabra *c* sea parte del contexto de una palabra target *w* como:\n",
    "\n",
    "$P(+|w,c) = \\sigma(\\vec{c}\\cdot\\vec{w}) = \\frac{1}{1+\\exp(-\\vec{c}\\cdot\\vec{w})}$, donde $\\sigma$ es la función sigmoide.\n",
    "\n",
    "Para el conjunto de palabras en una ventana de contexto de tamaño L, tenemos:\n",
    "\n",
    "$\\log P(+|w,c_{1:L}) = \\sum_{i=1}^{L}\\log\\sigma(\\vec{c_{i}}\\cdot\\vec{w})$\n",
    "\n",
    "La función de pérdida a minimizar por el clasificador viene dada por:\n",
    "\n",
    "$\\mathcal{L} = - \\left[\\log\\sigma(c_{pos}\\cdot{w}) + \\sum_{i=1}^{k}\\sigma(-c_{neg,i}\\cdot{w})\\right]$\n",
    "\n",
    "Esta función tiene por objeto maximizar la similitud entre la palabra target y sus palabras contexto $c_{pos}$, extraídas a partir de los ejemplos positivos, y a la vez minimizar la similitud entre la palabra target y las palabras que no pertenecen al contexto $c_{neg}$, extraídas a partir de los muestreos negativos.\n",
    "\n",
    "Para minimizar la función de pérdida, se utiliza el *stochastic gradient descent* algorithm. \n",
    "\n",
    "En conjunto, el algoritmo empieza con una inicialización aleatoria de las matrices $\\boldsymbol{W}$ y $\\boldsymbol{C}$, y realiza el entrenamiento utilizando *gradient descent* para modificar estas matrices con el fin de maximizar la función objetivo. Se acaban aprendiendo dos *embeddings* para cada palabra, el **target embedding** $\\boldsymbol{w_{i}}$ y el **context embedding** $\\boldsymbol{c\n",
    "_{i}}$, guardados en la matriz target $\\boldsymbol{W}$ y la matriz contexto $\\boldsymbol{C}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33732118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.1.2-cp39-cp39-macosx_10_9_x86_64.whl (24.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.0 MB 30.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 14.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /Users/galogonzalvo/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/galogonzalvo/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.20.3)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.1.2 smart-open-5.2.1\n"
     ]
    }
   ],
   "source": [
    "## Gensim is an open-source library for unsupervised topic modelling and NLP,\n",
    "## using modern statistical ML\n",
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e80bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model on google news to perform simple actions with word embeddings\n",
    "import gensim.downloader as api\n",
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "604a1f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model similarity between king and queen is 0.6510956883430481.\n",
      "Model similarity between king and potato is 0.09978463500738144.\n"
     ]
    }
   ],
   "source": [
    "#Obtain similarities from pre-trained model\n",
    "sim = model.similarity(\"king\", \"queen\")\n",
    "dif = model.similarity(\"king\",\"potato\")\n",
    "print (f\"Model similarity between king and queen is {sim}.\")\n",
    "print (f\"Model similarity between king and potato is {dif}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66a641dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('monarch', 0.7042065858840942),\n",
       " ('kings', 0.6780862808227539),\n",
       " ('princess', 0.6731551885604858),\n",
       " ('queens', 0.6679496765136719),\n",
       " ('prince', 0.6435247659683228)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Obtain top 5 similar words to a set of words\n",
    "model.most_similar([\"king\", \"queen\"],topn=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "905c12b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'air'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check which word does not fit to a list\n",
    "model.doesnt_match([\"summer\", \"fall\", \"spring\", \"air\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937e3282",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "1. Usa el modelo word2vec para hacer un raking de las siguientes 15 palabras según su similitud con las palabras \"man\" y \"woman\". Para cada par, imprime su similitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f856f76f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/b5/f8xxt6pd56n2g0ylq4cbm36m0000gn/T/ipykernel_940/2958979396.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mdic_man\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"man\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mdic_woman\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"woman\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "words = [\n",
    "\"wife\",\n",
    "\"husband\",\n",
    "\"child\",\n",
    "\"queen\",\n",
    "\"king\",\n",
    "\"man\",\n",
    "\"woman\",\n",
    "\"birth\",\n",
    "\"doctor\",\n",
    "\"nurse\",\n",
    "\"teacher\",\n",
    "\"professor\",\n",
    "\"engineer\",\n",
    "\"scientist\",\n",
    "\"president\"]\n",
    "\n",
    "\n",
    "dic_man = {}\n",
    "dic_woman = {}\n",
    "\n",
    "for word in words:\n",
    "    dic_man[word] = model.similarity(\"man\",word)\n",
    "    dic_woman[word] = model.similarity(\"woman\",word)\n",
    "\n",
    "sorted_man = {k: v for k, v in sorted(dic_man.items(), reverse=True, key=lambda item: item[1])}\n",
    "sorted_woman = {k: v for k, v in sorted(dic_woman.items(), reverse=True, key=lambda item: item[1])}\n",
    "\n",
    "print (list(sorted_man.keys()))\n",
    "print (list(sorted_woman.keys()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e97496",
   "metadata": {},
   "source": [
    "2. Completa las siguientes analogías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0eeb570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# man is to woman as king is to ...\n",
    "model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b214c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('school', 0.60170978307724)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nurse is to hospital as teacher is to ...\n",
    "model.most_similar(positive=[\"teacher\", \"hospital\"], negative=[\"nurse\"], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc6a299",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
