{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema de CBOW y skip-gram\n",
    "\n",
    "No tiene en cuenta el **orden de las palabras**, con lo que se puede perder el significado. Además tienen un gran **coste computacional y temporal**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neuronales recurrentes (RNN)\n",
    "\n",
    "Las RNNs procesan **secuencias** y retienen información en una especie de \"memoria\" (estado). Donde $x_{i}$ y $h_{i-1}$ actúan como input para el siguiente paso, donde se produce $h_{i}$, el output en cada iteración. El output se representa como un vector de números que representa toda la frase en su contexto. De esta manera, se tiene en cuenta el contexto de la frase para producir el siguiente output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"RNN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipos de RNNs\n",
    "\n",
    "- **Normal RNN**: Clasificación de imágenes (un input, un output).\n",
    "- **One-to-many**: Escribir el pie de una imagen (input: imágen, output: pie de imagen con muchas palabras).\n",
    "- **Many-to-one**: Clasificación de sentimientos (input: oración, output: sentimiento).\n",
    "- **Many-to-many**: Traducción de textos, clasificación de cada fotograma de un vídeo..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemas de las RNNs\n",
    "\n",
    "El **gradiente** indica el ajuste a realizar en los pesos con respecto a la variación en el error. Existen dos problemas con los gradientes:\n",
    "\n",
    "- **Gradientes explosivos**: Gradiente demasiado grande, el algoritmo asigna una importance exageradamente alta a las pesos. Tiene fácil solución truncando los gradientes.\n",
    "- **Gradientes desaparecidos**: Gradientes demasiado pequeños y el modelo deja de aprender o aprende muy despacio. Sucede a menudo cuando las secuencias son relativamente **largas**. Es más difícil de solucionar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-short term memory (LSTMs)\n",
    "\n",
    "Se plantea como una solución para retener en memoria palabras que aparecieron hace mucho tiempo (e.g. $x_{1}$ w.r.t. $x_{6}$) y no perder su información. Mantienen los valores de los gradientes suficientemente altos, realizando un entrenamiento más rápido y de mayor precisión. \n",
    "\n",
    "La idea es utilizar un vector a modo de peso ($\\epsilon~[0,1]$) que determinará cómo de importante es la información de los pasos anteriores. Los vectores se aplican en un sistema de *puertas*. La puerta de olvido se emprlea para dterminar la importancia del output anterior $h_{i-1}$, la puerta de entrada valora la importancia de un nuevo input $x_{i}$ y la puerta de salida le aplica un peso al output producido $h_{t}$.\n",
    "\n",
    "Otra alternativa son las llamadas **Gated Recurrent Units (GRUs)**, que tienen un rendimiento similar pero son computacionalmente más eficientes.\n",
    "\n",
    "El problema de las LSTMs es que los cálculos se hacen en serie, palabra por palabra, lo que ralentiza mucho el entrenamiento. Si la frase es muy larga, sigue siendo un problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de texto usando LSTMs con TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\r\n"
     ]
    }
   ],
   "source": [
    "## Download dataset\n",
    "\n",
    "!wget https://raw.githubusercontent.com/susanli2016/PyCon-Canada-2019-NLP-Tutorial/master/bbc-text.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Import necessary libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "## Import necessary libraries\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Access and visualize the dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbc-text.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "## Access and visualize the dataset\n",
    "df = pd.read_csv('bbc-text.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39minfo()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STOPWORDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m labels\u001b[38;5;241m.\u001b[39mappend(row[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     11\u001b[0m article \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mSTOPWORDS\u001b[49m:\n\u001b[1;32m     13\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m word \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m     article \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mreplace(token, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'STOPWORDS' is not defined"
     ]
    }
   ],
   "source": [
    "## Create the dataset\n",
    "\n",
    "articles = []\n",
    "labels = []\n",
    "\n",
    "with open('bbc-text.csv','r') as f:\n",
    "    reader = csv.reader(f,delimiter=',')\n",
    "    next(reader) # skip first line\n",
    "    for row in reader:\n",
    "        labels.append(row[0])\n",
    "        article = row[1]\n",
    "        for word in STOPWORDS:\n",
    "            token = ' ' + word + ' '\n",
    "            article = article.replace(token, ' ')\n",
    "        articles.append(article)\n",
    "        \n",
    "print(len(labels))\n",
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STOPWORDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# This could have been done directly with pandas\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mSTOPWORDS\u001b[49m:\n\u001b[1;32m      3\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mword\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'STOPWORDS' is not defined"
     ]
    }
   ],
   "source": [
    "# This could have been done directly with pandas\n",
    "for word in STOPWORDS:\n",
    "    df['text'] = df['text'].apply(lambda x: x.replace(' '+word+' ',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network parameters\n",
    "vocab_size = 5000 # This is the maximum number of words to keep\n",
    "embedding_dim = 64\n",
    "# Padding parameters (see later for padding definition)\n",
    "max_length = 200 # Maximum length of all sequences\n",
    "trunc_type = 'post' #pre or post. Pad (add zeros) either before or after each sequence.\n",
    "padding_type = 'post' #pre or post. Remove values larger than maximum_length either at the beginning or at the end of each sequence.\n",
    "oov_tok = '<OOV>' #Used to replace out-of-vocabulary words\n",
    "training_portion = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train/test split\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39mtraining_portion)\n\u001b[1;32m      4\u001b[0m X_train, X_test \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m][:train_size], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m][train_size:]\n\u001b[1;32m      5\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m][:train_size], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m][train_size:]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Train/test split\n",
    "train_size = int(df.shape[0]*training_portion)\n",
    "\n",
    "X_train, X_test = df['text'][:train_size], df['text'][train_size:]\n",
    "y_train, y_test = df['category'][:train_size], df['category'][train_size:]\n",
    "print (len(X_train), len(y_train))\n",
    "print (len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Create the tokenizer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizer\u001b[49m(num_words \u001b[38;5;241m=\u001b[39m vocab_size, oov_token\u001b[38;5;241m=\u001b[39moov_tok)\n\u001b[1;32m      3\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(X_train) \u001b[38;5;66;03m# Updates internal vocabulary based on a list of texts\u001b[39;00m\n\u001b[1;32m      4\u001b[0m word_index \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mword_index\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "## Create the tokenizer\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train) # Updates internal vocabulary based on a list of texts\n",
    "word_index = tokenizer.word_index # This returns a dictionary, where key->value = word->index. 0 is reserved for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Convert tokens of text corpus into a sequence of integers based on the word_index dictionary (embedding)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_sequences \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mtexts_to_sequences(X_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "## Convert tokens of text corpus into a sequence of integers based on the word_index dictionary (embedding)\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "#print (X_train[0])\n",
    "#print (train_sequences[0])\n",
    "#print (word_index['tv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding\n",
    "\n",
    "Cuando los textos no tienen el mismo tamaño, hemos de usar *padding*. Esta técnica permite que diferentes piezas de texto acaben con un vector del mismo tamaño, asignando ceros hasta que todas las piezas tengan el mismo tamaño. Esto es necesario para utilizar los textos como inputs para nuestra red neuronal. \n",
    "\n",
    "El padding tiene un tamaño máximo que puede ser el del texto más largo, o puede estar fijado por el usuario. En este segundo caso, se pierde información pero por otro lado puede hacer el algoritmo mucho más rápido, en el caso de que tengamos un texto muy muy largo junto a textos mucho más cortos, pues para los textos cortos el vector se vería lleno de ceros sin información útil, ralentizando el proceso. En ningún caso tendremos una longitud de padding mayor que la longitud de los vectores de las *hidden layers* de la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pad_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Create padded training data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_padded \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequences\u001b[49m(train_sequences,\n\u001b[1;32m      3\u001b[0m                              maxlen\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m      4\u001b[0m                              padding\u001b[38;5;241m=\u001b[39mpadding_type,\n\u001b[1;32m      5\u001b[0m                              truncating\u001b[38;5;241m=\u001b[39mtrunc_type\n\u001b[1;32m      6\u001b[0m                             )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;28mlen\u001b[39m(train_sequences[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;28mlen\u001b[39m(train_padded[\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pad_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "## Create padded training data\n",
    "train_padded = pad_sequences(train_sequences,\n",
    "                             maxlen=max_length,\n",
    "                             padding=padding_type,\n",
    "                             truncating=trunc_type\n",
    "                            )\n",
    "\n",
    "print (len(train_sequences[0]))\n",
    "print (len(train_padded[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Do the same for testing with the trained tokenizer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m validation_sequences \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mtexts_to_sequences(X_test)\n\u001b[1;32m      3\u001b[0m validation_padded \u001b[38;5;241m=\u001b[39m pad_sequences(validation_sequences,\n\u001b[1;32m      4\u001b[0m                                   maxlen\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m      5\u001b[0m                                   padding\u001b[38;5;241m=\u001b[39mpadding_type,\n\u001b[1;32m      6\u001b[0m                                   truncating\u001b[38;5;241m=\u001b[39mtrunc_type\n\u001b[1;32m      7\u001b[0m                                  )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "## Do the same for testing with the trained tokenizer\n",
    "validation_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "validation_padded = pad_sequences(validation_sequences,\n",
    "                                  maxlen=max_length,\n",
    "                                  padding=padding_type,\n",
    "                                  truncating=trunc_type\n",
    "                                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m##Codify labels\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m label_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizer\u001b[49m()\n\u001b[1;32m      3\u001b[0m label_tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(y_train)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#Since we have 5 categories, this will be transformed into integer numbers from 1 to 5\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "##Codify labels\n",
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(y_train)\n",
    "\n",
    "#Since we have 5 categories, this will be transformed into integer numbers from 1 to 5\n",
    "train_label_sequences = np.array(label_tokenizer.texts_to_sequences(y_train))\n",
    "test_label_sequences = np.array(label_tokenizer.texts_to_sequences(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     reverse_word_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m([(value,key) \u001b[38;5;28;01mfor\u001b[39;00m (key,value) \u001b[38;5;129;01min\u001b[39;00m word_index\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([reverse_word_index\u001b[38;5;241m.\u001b[39mget(i,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m text])\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[43mX_train\u001b[49m[\u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m (decode_article(train_padded[\u001b[38;5;241m10\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "## Decoded article (transform from list of numbers back to text)\n",
    "def decode_article(text):\n",
    "    reverse_word_index = dict([(value,key) for (key,value) in word_index.items()])\n",
    "    return ' '.join([reverse_word_index.get(i,'?') for i in text])\n",
    "print (X_train[10])\n",
    "print (decode_article(train_padded[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mversion\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.version.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Build the neural network\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m      4\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mEmbedding(vocab_size, embedding_dim),\n\u001b[1;32m      5\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mBidirectional(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLSTM(embedding_dim)),\n\u001b[1;32m      6\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(embedding_dim, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      7\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m6\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m ])\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "## Build the neural network\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
    "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
    "    tf.keras.layers.Dense(6, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python389jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
